[{"num": 1, "sprints": [{"id": 1, "start": "07.01.2020", "end": "21.01.2020", "text": "<h2 class='report_headline'>First Report and overview</h2><br><p class='report_paragraph'>The first thing I wanted to accomplish was to put up a shell for the blog. Having recently taught myself React and Typescript, this was a great opportunity to try it out. The backend is extremely simple, and for now it is just a node server using express js. The idea as of now is just to fetch the entire dataset, and pass the necessary props down, but if more complexity is added it might be a good idea to only fetch the contents of the reports that are requested. I might also want to implement more CRUD-operations in the future. I am a bit unsure how much code I should put up here, as my entire project cannot be open source for obvious reasons. I will have to be a bit restrictive anyhow.</p><br><p class='report_paragraph'>The next thing I did was to implement a python parser that goes through a text that has been marked up with an extremely simple markup language, and creates a report from it. So far it only deals with headlines and texts, so that will have to be improved later.</p><br><p class='report_paragraph'>I also hope to complete a little bit of styling to make it look a bit nicer.</p><br><h4 class='report_subheader'>Notes and pointers</h4><br><p class='report_paragraph'>Reading the first few research articles was enlightening. A lot of things make a lot of sense, other things don't. The overarching question of how to improve dialogue systems is such a complex and loaded one that has to be modularized to make sense. Making a bit more sense of the question, one can ask, what makes a dialogue system good? Unfortunately, that question is unanswerable to almost the same degree as the previous one. Context has to be applied for any form of measurement to make sense. If humans were completely rational beings who all hold the same goals and ideas, it could be simple. One can for example measure how many questions a dialogue system can process, understand, and make an action towards answering. However, a user might misinterpret the answer that the system has provided, which renders the action practically useless in that certain setting, even though the system performed the correct action for general use.</p><br><p class='report_paragraph'>If one uses a more rational approach however, and rather ask the question 'how does my system perform, and how does that performance differ from how I want it to work?', it is much easier to evaluate. Though it does not make it easier to solve, as a dialogue system is built from different modules. For example, there has to be a part of the system with the responsibility of understanding what the user is saying. Then the system has to break that down into something measurable, in order to evaluate what it can do about it. Now this system is an entirely different beast on its own, there has to be a way to track the state of the conversation, there has to be a way of knowing which actions are available to the system at any given point, and a way to evaluate which action is most suitable to move to a different state that makes the most sense for the situation. Finally, there are infinitely many ways of responding to the utterance of the user. Some are obviously more suitable than others, and that entirely depends on the situation.</p><br><p class='report_paragraph'>In other words, context matters. One cannot expect a system to be able to respond to everything, or even reasonably to utterances clearly outside of the scope of the context. The context is extremely important. Therefore, I will completely ignore it for now. It does not have the theoretical importance before a few other things are properly laid out and understood.</p><br><h4 class='report_subheader'>Weak supervision</h4><br><p class='report_paragraph'>For natural language processing, machine learning is extremely useful for building models. There are many ways of creating models using machine learning approaches, however there are a few that stand out. First of all, supervised learning makes the most sense for building a model that is to be used for semantic extraction. If we can tell the model which examples of the data we've got are right and which ones are wrong for what we are looking for, it can obviously learn quicker than if we let it figure it out by itself. Within supervised machine learning methods deep learning is leading the competition to be 'state of the art'.</p><br><p class='report_paragraph'>Deep learning is extremely effective for semantic analysis. There are problems connected to it though, most of them practical. Given infinite data, infinite computing power, and infinite time, deep learning is unbeatable. Those things are however almost never a given. We often have a lot of data for a given context, but we rarely have enough labelled data. We need to tell a supervised learning algorithm which examples should be classified in some way, and which should be classified in another. Generally, this is done through the so-called gold standard labels, where by hand each element in a data set is labeled. This takes a lot of time, and for some contexts, it can be very expensive.</p><br><p class='report_paragraph'>There have been several attempts to solve this in recent years. Williams et al. (2015) proposes the paradigm of interactive learning, where a domain expert can label the elements of a certain perplexity and propagating to similar elements, spending a fraction of the time it requires to hand label a dataset. While clever, there are some issues with the approach. If more data is added in the future of simlar context but with differently structured content, the propogation might have some issues. It seems that a lot of problems could be solved with a scalable, at least semi-automatic labelling process with minimal need for continuous domain expertise.</p><br><p class='report_paragraph'>Weak supervision is a relatively new paradigm that seeks to solve this problem of lack of domain knowledge, by approaching the problem in a different way. What if, instead of operating from a ground truth, one could write functions to automatically label the data? That would be pretty neat. This is of course not as accurate as hand labeling, but it is a lot cheaper in all measurable ways. It obviously requires some heuristic knowledge of the data, in order to create these functions, but it makes the process scalable in a whole other way. The evaluation of the elements differ slightly from evaluation by hand, as there will be cases where a function either cannot decide on a label if the heuristic rules are not sufficient. There will also be disagreements between the functions, which will affect the level of noise. We can then use the information about these disagreements to learn dependencies between the different functions, as to which functions fixes or reinforces others.</p><br>"}, {"id": 2, "start": "22.01.2020", "end": "05.02.2020", "text": "<h2 class='report_headline'>Labeling functions</h2><br><p class='report_paragraph'>Having established the motivation for and the general principles of weak learning in the previous report, this first part will feature my (believed) knowledge about labeling functions. I will try to explain it having read about the tool Snorkel, which attempts to use labeling functions (Ratner et al. 2017).</p><br><p class='report_paragraph'>Snorkel differentiates between two types of labeling functions. The most important type is pattern based, where labeling functions are generated from patterns in the text. The least important one will not be of much discussion, as they are manual functions that can be applied with some domain knowledge. The problem with pattern based labeling functions have traditionally been that they require more analytical skill from the annotator, and that they require more time than active learning (Zaidan & Eisner, 2008). A way of dealing with this is through feature annotation, where annotators pick out which parts of the data that made them annotate it the way they did, through some mean that is abstract from the data. For example circling around a street sign on an image. Then patterns can be generated based on this information. (ibid.)</p><br><p class='report_paragraph'>The generation of patterns is of importance. Generally a labeling function will look for a pattern involving some entities in a text, with the text around functioning as information about the relationship between the entities. Gupta & Manning (2014) discusses how studies up until then had not focused on the unlabeled entities, but entirely dismissing them as negatives or are ignored by the existing pattern scoring measures. They propose to use unsupervised learning on both the general and the contrasting domain specific text, while making use of some statistical methods to boot. Using these measures they extracted more entities that otherwise would have been ignored, which would be a shame of course. (ibid.)</p><br><p class='report_paragraph'>It is also worth to mention distant supervision when discussing patterns and entities. In short, distant supervision makes use of the techniques discussed over, by assuming that any text with some entities involved will have some useful information regarding their relation. Some implementations have involved using topic models to discriminate between entries that express a relation and those that are ambiguous, while others have focused on dealing with overlapping relations, and connecting relations with identical actors to each other (Alfonseca et al. 2012)(Hoffman et al. 2011).</p><br><h4 class='report_subheader'>Literature</h4><br><p class='report_paragraph'>Ratner, A. et al. 2017, Snorkel: Rapid Training Data Creation with Weak Supervision</p><br><p class='report_paragraph'>Alfonseca, E. et al. 2012, Pattern learning for relation extraction with a hierarchical topic model</p><br><p class='report_paragraph'>Gupta, S. & Manning, C.D. 2014, Improved pattern learning for bootstrapped entity extraction</p><br><p class='report_paragraph'>Hoffman, R. et al, 2011, Knowledge-based weak supervision for information extraction of overlapping relations</p><br><p class='report_paragraph'>Zaidan, O.F. & Eisner, J. 2008, Modeling annotators: A generative approach to learning from annotator rationales</p><br>"}, {"id": 3, "start": "06.02.2020", "end": "20.02.2020", "text": "<h2 class='report_headline'>Dialogue state</h2><br><p class='report_paragraph'>Something that has always seperated us from machines, is our ability to infer information from seemingly nowhere. We can come up with information that has nothing to do with a conversation we're having, and the other person can perfectly understand it, and put the information into context. For computers, this is unthinkable. Models, either rule-based or otherwise, are shown a world, and are made to completely understand everything about that world. But the second they are shown external information, they cannot infer any sure and doubtless knowledge. This context, and knowledge of the potential mutations of the context is referred to as the dialogue state.</p><br><p class='report_paragraph'>Rule-based systems absolutely cannot do this by themselves. If the condition of any rules are not satisfied, then a rule-based system cannot produce a response. What most people do to counteract this, is to make use of probabilistic models. This way, you can make sure that a rule will fire, even if the conditions are not fully satisfied. By breaking the text into vectors and performing statistical analysis on them, one can find the rule that is the most probable to be correct for the context. There will be cases where the probability of any condition being satisfied is extremely low, and in such a case, one can have a standard response to attempt to make the user clarify their request.</p><br><h4 class='report_subheader'>Bayesian networks</h4><br><p class='report_paragraph'>While currently not the \"state of the art\"-method of building dialogue systems around, Baysian Networks are great for displaying the functionality of the dialogue state as they can be represented graphically (Lison, 2014). Theoretically bayesian networks are dependency networks of probability distributions that describe how the system will react to certain inputs. The network consists of three types of nodes; chance-, decision-, and utility nodes, and directed edges between them that represent the dependency between them. The chance nodes represent the conditional probability distributions, decision nodes correspond to variables under control of the system, and utility nodes are coupled with utility functions that associates values in node parents with a specific utility. An example of a bayesian network:</p><br><img src=https://adriyst.github.io/report_endpoint/baynet.png class='inline_image' alt='Bayesian Network' /><br><p class='report_paragraph'>While Bayesian networks are handy and easy to visualize, they are still to rigid in their nature to be optimal. Serban et al. (2015) brought dialogue systems into the Neural age when he showed that end-to-end dialogue systems through multiple neural models could provide a system that could learn the dialogue state, and would thus not suffer from human ineptness when it comes to understanding data.</p><br>"}]}]